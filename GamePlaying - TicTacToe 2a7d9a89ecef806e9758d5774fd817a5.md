# GamePlaying - TicTacToe

# I. T√¨m hi·ªÉu

## 1. M·ª•c ti√™u

‚Ä¢ Hi·ªán th·ª±c game playing agent cho m·ªôt tr√≤ ch∆°i t·ª± ch·ªçn (d·∫°ng ƒë·ªëi kh√°ng).

‚Ä¢ N√¢ng cao k·ªπ nƒÉng l·∫≠p tr√¨nh, gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ.

## 2. Y√™u c·∫ßu

M·ªói nh√≥m ch·ªçn m·ªôt tr√≤ ch∆°i kh√¥ng d·ªÖ - tr√≤ ch∆°i n·∫øu sinh c√¢y ph·∫£i c√≥ h·ªá s·ªë nh√°nh cao v√† ƒë·ªô s√¢u
c·ªßa c√¢y t·ª´ 30 tr·ªü l√™n (VD: c·ªù t∆∞·ªõng, c·ªù vua, c·ªù v√¢y, ...) ƒë·ªÉ hi·ªán th·ª±c b·∫±ng gi·∫£i thu·∫≠t Minimax v√†
gi·∫£i thu·∫≠t ‚Äì m√¥ h√¨nh H·ªçc m√°y (b·∫•t k·ª≥), v·ªõi y√™u c·∫ßu c·ª• th·ªÉ nh∆∞ sau:

1. Agent ph·∫£i ch∆°i ƒë√∫ng lu·∫≠t.
2. Minimax th·∫Øng ƒë∆∞·ª£c agent ch∆°i ng·∫´u nhi√™n v·ªõi t·ªâ l·ªá 90%.
3. H·ªçc m√°y th·∫Øng ƒë∆∞·ª£c agent ch∆°i ng·∫´u nhi√™n v·ªõi t·ªâ l·ªá 60%.
4. Nh√≥m t·∫°o video thuy·∫øt tr√¨nh (t·ªëi ƒëa 15 ph√∫t) v·ªÅ tr√≤ ch∆°i, hi·ªán th·ª±c c·ªßa nh√≥m, v√† k·∫øt qu·∫£
c·ªßa c√°c gi·∫£i thu·∫≠t.

ƒê·ªÅ b√†i n√™u r√µ c√≥ **3 lo·∫°i agent kh√°c nhau** m√† nh√≥m ph·∫£i hi·ªán th·ª±c:

| Lo·∫°i Agent | M√¥ t·∫£ | Vai tr√≤ trong b√†i |
| --- | --- | --- |
| üé≤ **Random Agent** | Ch∆°i ng·∫´u nhi√™n ‚Äî ch·ªçn b·∫•t k·ª≥ n∆∞·ªõc h·ª£p l·ªá n√†o (legal move) m·ªôt c√°ch random. | L√† ƒë·ªëi th·ªß ki·ªÉm th·ª≠ (benchmark) ƒë·ªÉ ƒëo hi·ªáu qu·∫£. |
| üß† **Minimax Agent** | Agent d√πng thu·∫≠t to√°n **Minimax (ho·∫∑c Alpha-Beta pruning)** ƒë·ªÉ ch·ªçn n∆∞·ªõc t·ªëi ∆∞u d·ª±a tr√™n m√¥ ph·ªèng c√¢y tr·∫°ng th√°i. | Ph·∫£i th·∫Øng Random Agent ‚â• **90%**. |
| ü§ñ **Learning Agent** | Agent d√πng **m·ªôt m√¥ h√¨nh h·ªçc m√°y** (VD: Q-learning, Neural Network, Decision Tree, v.v.) ƒë·ªÉ t·ª± h·ªçc c√°ch ch∆°i. | Ph·∫£i th·∫Øng Random Agent ‚â• **60%**. |

### DEMO with Random Agent

[GitHub - Long-noop/tictactoe-pygame: A well known two player game made using Python and Pygame.](https://github.com/Long-noop/tictactoe-pygame)

### **Agent Classes**

C√≥ th·ªÉ thi·∫øt k·∫ø nh∆∞ sau:

```python
class Agent:
    def __init__(self, symbol):
        self.symbol = symbol  # 'X' ho·∫∑c 'O'
    def get_move(self, state):
        pass  # ƒë∆∞·ª£c override ·ªü t·ª´ng lo·∫°i agent
```

R·ªìi k·∫ø th·ª´a:

```python
class RandomAgent(Agent):
    def get_move(self, state):
        moves = get_legal_moves(state)
        return random.choice(moves)
```

```python
class MinimaxAgent(Agent):
    def get_move(self, state):
        best_move = minimax(state, depth, alpha, beta, True)
        return best_move
```

```python
class LearningAgent(Agent):
    def get_move(self, state):
        # D·ª±a v√†o model h·ªçc m√°y ƒë√£ train
        return model.predict_best_move(state)
```

---

## 3. X√°c ƒë·ªãnh quy t·∫Øc c∆° b·∫£n

- B√†n c·ªù: 9 **√ó 9 = 81 √¥.**
- M·ªói l∆∞·ª£t: ng∆∞·ªùi ch∆°i ƒë·∫∑t 1 d·∫•u (X ho·∫∑c O) v√†o **1 √¥ tr·ªëng**.
- ƒêi·ªÅu ki·ªán th·∫Øng (th√¥ng th∆∞·ªùng trong b·∫£n m·ªü r·ªông): 5 **li√™n ti·∫øp** (ngang, d·ªçc, ch√©o).
- Kh√¥ng c√≥ ƒÉn qu√¢n hay lo·∫°i b·ªè qu√¢n (t·ª©c l√† kh√¥ng c√≥ ‚Äúback-move‚Äù).

---

### ƒê·ªô s√¢u t·ªëi ƒëa (max depth)

**ƒê·ªô s√¢u c√¢y t√¨m ki·∫øm = s·ªë n∆∞·ªõc ƒëi t·ªëi ƒëa trong m·ªôt v√°n.**

- ·ªû Tic-Tac-Toe 9√ó9: t·ªïng c·ªông 81 **√¥**,
- M·ªói l∆∞·ª£t ƒëi chi·∫øm 1 √¥,
    
    ‚Üí N·∫øu kh√¥ng ai th·∫Øng s·ªõm, th√¨ **max depth = 81.**
    

`maxDepth = 81` (trong tr∆∞·ªùng h·ª£p v√°n k√©o d√†i ƒë·∫øn h·∫øt b√†n).

---

### Branching Factor (h·ªá s·ªë nh√°nh)

a. ƒê·ªãnh nghƒ©a:

H·ªá s·ªë nh√°nh trung b√¨nh (average branching factor, k√Ω hi·ªáu *b*) l√† **s·ªë l∆∞·ª£ng n∆∞·ªõc ƒëi h·ª£p l·ªá trung b√¨nh t·∫°i m·ªói tr·∫°ng th√°i**.

b. Ph√¢n t√≠ch:

- Ban ƒë·∫ßu: c√≥ 81 n∆∞·ªõc ƒëi h·ª£p l·ªá.
- Sau 1 n∆∞·ªõc: c√≤n 80, r·ªìi 79, ‚Ä¶ cho ƒë·∫øn 1.
- Trung b√¨nh ta c√≥ th·ªÉ x·∫•p x·ªâ:  $b = (81+1)/2=41$
    
    41 **n∆∞·ªõc ƒëi h·ª£p l·ªá trung b√¨nh m·ªói l∆∞·ª£t**.
    

> ‚úÖ K·∫øt lu·∫≠n:
> 
> 
> `Branching factor ‚âà 41`
> 
> `Max depth = 81`
> 

---

### So s√°nh v·ªõi c√°c phi√™n b·∫£n kh√°c

| Phi√™n b·∫£n | √î | Branching Factor (‚âà) | Max Depth | Nh·∫≠n x√©t |
| --- | --- | --- | --- | --- |
| 3√ó3 | 9 | 4.5 | 9 | Qu√° nh·ªè, d·ªÖ gi·∫£i h·∫øt c√¢y |
| 4√ó4 | 16 | 8 | 16 | Trung b√¨nh |
| 5√ó5 | 25 | 12.5 | 25 | Trung b√¨nh |
| **6√ó6** | **36** | **18** | **36** | ‚úÖ **ƒê·∫°t y√™u c·∫ßu ƒë·ªÅ b√†i (ƒë·ªô s√¢u ‚â• 30, h·ªá s·ªë nh√°nh cao)** |

---

# II. MiniMax Agent

## 4. Gi·∫£i thu·∫≠t Minimax

N·∫øu ta d√πng Minimax **kh√¥ng pruning**, s·ªë tr·∫°ng th√°i c·∫ßn duy·ªát s·∫Ω l√†: $41^{81}$ (r·∫•t l·ªõn, kh√¥ng th·ªÉ t√≠nh h·∫øt)

‚Üí V√¨ v·∫≠y **b·∫Øt bu·ªôc** ph·∫£i d√πng **alpha-beta pruning** + **gi·ªõi h·∫°n ƒë·ªô s√¢u (depth limit)**, v√≠ d·ª•:

- Depth limit = 5 ho·∫∑c 6.
- Evaluation function (heuristic) ƒë√°nh gi√° t·∫°m th·ªùi.

---

### √ù t∆∞·ªüng t·ªïng qu√°t

**Minimax** gi√∫p agent ch·ªçn n∆∞·ªõc ƒëi t·ªëi ∆∞u, gi·∫£ ƒë·ªãnh r·∫±ng:

- Agent (X) lu√¥n **c·ªë g·∫Øng t·ªëi ƒëa h√≥a** ƒëi·ªÉm (maximize).
- ƒê·ªëi th·ªß (O) lu√¥n **c·ªë g·∫Øng t·ªëi thi·ªÉu h√≥a** ƒëi·ªÉm (minimize).

C√¢y tr√≤ ch∆°i ƒë∆∞·ª£c sinh ra b·ªüi c√°c tr·∫°ng th√°i b√†n c·ªù ‚Üí n∆∞·ªõc ƒëi h·ª£p l·ªá ‚Üí tr·∫°ng th√°i m·ªõi.

### Alpha‚ÄìBeta pruning

D√πng ƒë·ªÉ **c·∫Øt b·ªè nh√°nh kh√¥ng c·∫ßn thi·∫øt**:

- `Œ±` = gi√° tr·ªã t·ªët nh·∫•t (max) m√† **ng∆∞·ªùi ch∆°i MAX** c√≥ th·ªÉ ƒë·∫£m b·∫£o.
- `Œ≤` = gi√° tr·ªã t·ªët nh·∫•t (min) m√† **ng∆∞·ªùi ch∆°i MIN** c√≥ th·ªÉ ƒë·∫£m b·∫£o.

N·∫øu t·∫°i b·∫•t k·ª≥ ƒëi·ªÉm n√†o `Œ± ‚â• Œ≤`, ta **c·∫Øt b·ªè (prune)** nh√°nh c√≤n l·∫°i.

---

## 5. ƒê·ªãnh nghƒ©a kh√¥ng gian tr·∫°ng th√°i (State Space Definition)

### 5.1.  C·∫•u tr√∫c state:

M·ªôt state g·ªìm:

```python
class GameState:
    def __init__(self, board, current_player):
        self.board = board          # ma tr·∫≠n 9x9
        self.current_player = current_player  # 'X' ho·∫∑c 'O'

```

### Kh√¥ng gian tr·∫°ng th√°i:

L√† **t·∫≠p h·ª£p t·∫•t c·∫£ c√°c tr·∫°ng th√°i h·ª£p l·ªá** c√≥ th·ªÉ sinh ra t·ª´ state ban ƒë·∫ßu theo lu·∫≠t ch∆°i:

[

S = { s0, s1, s2, ‚Ä¶, sn }

]

Trong ƒë√≥:

- `s_0`: tr·∫°ng th√°i ban ƒë·∫ßu (initial state)
- `s_n`: tr·∫°ng th√°i k·∫øt th√∫c (terminal state)
- `s_{i+1}` ƒë∆∞·ª£c sinh ra t·ª´ `s_i` b·∫±ng m·ªôt n∆∞·ªõc ƒëi h·ª£p l·ªá (legal move).

---

### 5.2. Initial state (tr·∫°ng th√°i ban ƒë·∫ßu)

```python
def initial_state():
    board = [["." for _ in range(6)] for _ in range(6)]
    return GameState(board, current_player="X")

```

---

### 5.3. Terminal state (tr·∫°ng th√°i k·∫øt th√∫c)

M·ªôt tr·∫°ng th√°i l√† **terminal** n·∫øu:

- M·ªôt ng∆∞·ªùi ch∆°i th·∫Øng (c√≥ 4 li√™n ti·∫øp), ho·∫∑c
- Kh√¥ng c√≤n √¥ tr·ªëng (h√≤a).

```python
def is_terminal(state):
    return check_winner(state.board, "X") or check_winner(state.board, "O") \
           or all(cell != "." for row in state.board for cell in row)

```

---

### 5.4. Legal moves (c√°c h√†nh ƒë·ªông h·ª£p l·ªá)

C√°c n∆∞·ªõc ƒëi h·ª£p l·ªá l√† **c√°c √¥ tr·ªëng** c√≤n l·∫°i tr√™n b√†n:

```python
def get_legal_moves(state):
    return [(r, c) for r in range(6) for c in range(6) if state.board[r][c] == "."]

```

---

### 5.5. Transition model (m√¥ h√¨nh chuy·ªÉn tr·∫°ng th√°i)

√Åp d·ª•ng m·ªôt h√†nh ƒë·ªông `move = (r, c)` v√†o state ‚Üí sinh ra state m·ªõi:

```python
def result(state, move):
    r, c = move
    new_board = [row[:] for row in state.board]
    new_board[r][c] = state.current_player
    next_player = "O" if state.current_player == "X" else "X"
    return GameState(new_board, next_player)

```

---

### 5.6. Evaluation Function (ƒê√°nh gi√° tr·∫°ng th√°i)

H√†m n√†y ∆∞·ªõc l∆∞·ª£ng **ƒë·ªô t·ªët** c·ªßa m·ªôt tr·∫°ng th√°i *khi ch∆∞a k·∫øt th√∫c game*, d√πng ƒë·ªÉ:

- So s√°nh c√°c tr·∫°ng th√°i.
- Alpha‚ÄìBeta pruning.

M·ª•c ti√™u:

- Gi√° tr·ªã c√†ng cao ‚Üí c√†ng c√≥ l·ª£i cho ng∆∞·ªùi ch∆°i X.
- Gi√° tr·ªã c√†ng th·∫•p ‚Üí c√†ng c√≥ l·ª£i cho ng∆∞·ªùi ch∆°i O.

---

5.6.1 Ph∆∞∆°ng ph√°p c∆° b·∫£n: ‚Äúƒê·∫øm chu·ªói li√™n ti·∫øp‚Äù

V·ªõi b√†n 9x9 v√† lu·∫≠t 5-in-a-row, ta xem:

| Th√†nh ph·∫ßn | M√¥ t·∫£ | G·ª£i √Ω tr·ªçng s·ªë |
| --- | --- | --- |
| `open_fours` | S·ªë l∆∞·ª£ng chu·ªói 4 qu√¢n li√™n ti·∫øp c√≥ 2 ƒë·∫ßu m·ªü | +1000 |
| `open_threes` | S·ªë l∆∞·ª£ng chu·ªói 3 qu√¢n li√™n ti·∫øp c√≥ 2 ƒë·∫ßu m·ªü | +100 |
| `blocked_fours` | Chu·ªói 4 qu√¢n b·ªã ch·∫∑n 1 ƒë·∫ßu | +300 |
| `two_in_row` | C·∫∑p qu√¢n li√™n ti·∫øp | +10 |

T∆∞∆°ng t·ª± cho ƒë·ªëi th·ªß ‚Üí tr·ª´ ƒëi·ªÉm.

---

5.6.2. C√†i ƒë·∫∑t h√†m ƒë√°nh gi√° 

```python
# M√£ tham kh·∫£o √°p d·ª•ng cho map 6x6, s·ª≠a l·∫°i theo ƒë√∫ng y√™u c·∫ßu map 9x9

def evaluate_line(line, player):
    opp = "O" if player == "X" else "X"
    score = 0
    if opp not in line:
        count = line.count(player)
        if count == 2:
            score += 10
        elif count == 3:
            score += 50
        elif count >= 4:
            score += 1000
    return score

def evaluate_state(state, player="X"):
    board = state.board
    total_score = 0

    # h√†ng ngang
    for r in range(6):
        for c in range(6 - 4 + 1):
            line = [board[r][c + i] for i in range(4)]
            total_score += evaluate_line(line, player)
            total_score -= evaluate_line(line, "O" if player == "X" else "X")

    # h√†ng d·ªçc
    for c in range(6):
        for r in range(6 - 4 + 1):
            line = [board[r + i][c] for i in range(4)]
            total_score += evaluate_line(line, player)
            total_score -= evaluate_line(line, "O" if player == "X" else "X")

    # ch√©o xu√¥i & ng∆∞·ª£c
    for r in range(6 - 4 + 1):
        for c in range(6 - 4 + 1):
            diag1 = [board[r + i][c + i] for i in range(4)]
            diag2 = [board[r + 3 - i][c + i] for i in range(4)]
            total_score += evaluate_line(diag1, player)
            total_score -= evaluate_line(diag1, "O" if player == "X" else "X")
            total_score += evaluate_line(diag2, player)
            total_score -= evaluate_line(diag2, "O" if player == "X" else "X")

    return total_score

```

---

T·ªïng quan m·ªëi quan h·ªá gi·ªØa c√°c th√†nh ph·∫ßn

| Th√†nh ph·∫ßn | H√†m / L·ªõp t∆∞∆°ng ·ª©ng | Vai tr√≤ |
| --- | --- | --- |
| **State** | `GameState` | M√¥ t·∫£ t√¨nh tr·∫°ng hi·ªán t·∫°i |
| **Initial state** | `initial_state()` | B·∫Øt ƒë·∫ßu game |
| **Legal moves** | `get_legal_moves(state)` | Sinh h√†nh ƒë·ªông h·ª£p l·ªá |
| **Transition** | `result(state, move)` | Sinh state con |
| **Terminal** | `is_terminal(state)` | Ki·ªÉm tra k·∫øt th√∫c |
| **Evaluation** | `evaluate_state(state)` | ∆Ø·ªõc l∆∞·ª£ng ƒë·ªô t·ªët c·ªßa state |

---

## 6. C√°ch t√≠ch h·ª£p v√†o Minimax

Sau khi c√≥ t·∫•t c·∫£ c√°c th√†nh ph·∫ßn tr√™n, ta tri·ªÉn khai:

```python
def minimax(state, depth, alpha, beta, maximizing_player, root_player):
    if is_terminal(state) or depth == 0:
        return evaluate_state(state, root_player)

    if maximizing_player:
        max_eval = -float("inf")
        for move in get_legal_moves(state):
            eval = minimax(result(state, move), depth-1, alpha, beta, False, root_player)
            max_eval = max(max_eval, eval)
            alpha = max(alpha, eval)
            if beta <= alpha:
                break  # pruning
        return max_eval
    else:
        min_eval = float("inf")
        for move in get_legal_moves(state):
            eval = minimax(result(state, move), depth-1, alpha, beta, True, root_player)
            min_eval = min(min_eval, eval)
            beta = min(beta, eval)
            if beta <= alpha:
                break
        return min_eval

```

---

# III. ML Agent

## C√°c h∆∞·ªõng ML kh·∫£ thi

C√≥ **3 h∆∞·ªõng ch√≠nh**, t√πy ƒë·ªô ph·ª©c t·∫°p mu·ªën tri·ªÉn khai:

| C·∫•p ƒë·ªô | Ph∆∞∆°ng ph√°p | M√¥ t·∫£ | ∆Øu ƒëi·ªÉm |
| --- | --- | --- | --- |
| **C∆° b·∫£n**  | **Q-learning (Reinforcement Learning)** | Agent t·ª± h·ªçc qua th·ª≠-sai, c·∫≠p nh·∫≠t b·∫£ng gi√° tr·ªã Q(s, a). | D·ªÖ hi·ªÉu, kh√¥ng c·∫ßn d·ªØ li·ªáu s·∫µn. |
| **Trung b√¨nh** | **Supervised Learning (H·ªçc c√≥ gi√°m s√°t)** | Train m√¥ h√¨nh (MLP, SVM, Decision Tree, v.v.) d·ª± ƒëo√°n n∆∞·ªõc ƒëi t·ªët nh·∫•t d·ª±a tr√™n d·ªØ li·ªáu t·ª´ Minimax ho·∫∑c chuy√™n gia. | T·∫≠n d·ª•ng d·ªØ li·ªáu, d·ªÖ ki·ªÉm so√°t. |
| **N√¢ng cao** | **Deep Reinforcement Learning** | D√πng Neural Network l√†m h√†m Q ho·∫∑c Policy (nh∆∞ AlphaZero). | C·ª±c m·∫°nh, nh∆∞ng ph·ª©c t·∫°p. |

## 1. Q-Learning

[Q-Learning in Reinforcement Learning - GeeksforGeeks](https://www.geeksforgeeks.org/machine-learning/q-learning-in-python/)

### 1.1 Q-Learning c∆° b·∫£n

### a. Kh√°i ni·ªám

Q-learning h·ªçc h√†m gi√° tr·ªã:

$Q(s,a)=$ gi√° tr·ªã  k·ª≥ v·ªçng n·∫øu th·ª±c hi·ªán h√†nh ƒë·ªông a trong tr·∫°ng th√°i s

### b. C·∫≠p nh·∫≠t Q

Sau m·ªói l∆∞·ª£t ƒëi:

$Q(s, a) = R(s, a) + Œ≥ * max_a' Q(s', a')$

Trong ƒë√≥:

- `Œ±` = learning rate
- `Œ≥` = discount factor
- `r` = reward (th∆∞·ªüng/ph·∫°t)
- `s'` = tr·∫°ng th√°i sau khi ƒëi n∆∞·ªõc `a`

---

### 1.2 Quy tr√¨nh h·ªçc

1. **Kh·ªüi t·∫°o:**
    
    ```python
    Q = {}  # dictionary l∆∞u gi√° tr·ªã Q[(state, move)]
    ```
    
2. **Ch·∫°y nhi·ªÅu v√°n hu·∫•n luy·ªán:**
    - Agent ch∆°i v·ªõi random agent ho·∫∑c self-play.
    - Sau m·ªói l∆∞·ª£t: c·∫≠p nh·∫≠t `Q(s, a)`.
3. **Reward:**
    
    
    | T√¨nh hu·ªëng | Reward |
    | --- | --- |
    | Th·∫Øng | +1 |
    | Thua | -1 |
    | H√≤a | 0 |
    | N∆∞·ªõc ƒëi h·ª£p l·ªá trung gian | 0 |
4. **Ch·ªçn h√†nh ƒë·ªông (Exploration/Exploitation):**
    
    ```python
    if random() < epsilon:
        move = random.choice(legal_moves)
    else:
        move = argmax(Q[state, move])
    ```
    
5. **Sau khi h·ªçc ƒë·ªß s·ªë v√°n**, gi·∫£m `epsilon` ‚Üí agent b·∫Øt ƒë·∫ßu **ch∆°i ‚Äúth√¥ng minh‚Äù h∆°n**.

---

1. V√≠ d·ª• code khung (Python)

```python
import random
from collections import defaultdict

Q = defaultdict(float)
alpha = 0.1
gamma = 0.9
epsilon = 0.2

def choose_action(state, legal_moves):
    if random.random() < epsilon:
        return random.choice(legal_moves)
    else:
        q_values = [Q[(state, move)] for move in legal_moves]
        return legal_moves[q_values.index(max(q_values))]

def update_Q(state, action, reward, next_state, next_legal_moves):
    next_q = max([Q[(next_state, a)] for a in next_legal_moves], default=0)
    Q[(state, action)] += alpha * (reward + gamma * next_q - Q[(state, action)])

```

---

### 1.3 ƒê√°nh gi√° tr·∫°ng th√°i (state representation)

V√¨ b√†n Tic-Tac-Toe 9√ó9 kh√° l·ªõn, ta c·∫ßn **bi·ªÉu di·ªÖn tr·∫°ng th√°i ƒë∆°n gi·∫£n**:

- Encode b√†n c·ªù th√†nh chu·ªói: `"XOXO....O"` (81 k√Ω t·ª±)
- Ho·∫∑c tuple: `('X', 'O', ' ', 'X', ...)`
- C√≥ th·ªÉ m√£ h√≥a `X = 1, O = -1, tr·ªëng = 0`.

## Supervised Learning

[Supervised and Unsupervised learning - GeeksforGeeks](https://www.geeksforgeeks.org/machine-learning/supervised-unsupervised-learning/)

N·∫øu c√≥ th·ªùi gian:

- D√πng **d·ªØ li·ªáu Minimax** (v·ªõi depth nh·ªè) ƒë·ªÉ t·∫°o dataset:
    
    `input = state`, `label = best_move`.
    
- Train model ƒë∆°n gi·∫£n (MLP ho·∫∑c DecisionTreeClassifier) ƒë·ªÉ h·ªçc c√°ch b·∫Øt ch∆∞·ªõc Minimax.
- Sau ƒë√≥ ƒë·ªÉ agent d√πng model n√†y ƒë·ªÉ ch·ªçn n∆∞·ªõc ƒëi.

## **Ph√¢n chia c√¥ng vi·ªác (deadline n·ªôp th·∫ßy 12/12/2025)**

| Task | Th√†nh vi√™n | Nhi·ªám v·ª• ch√≠nh | M·ª•c ti√™u | Deadline |
| --- | --- | --- | --- | --- |
| **1. Random Agent (ƒë√£ xong)** | Long | - Sinh n∆∞·ªõc ƒëi h·ª£p l·ªá ng·∫´u nhi√™n, ƒë·∫£m b·∫£o lu·∫≠t ch∆°i
- Vi·∫øt **giao di·ªán m√¥ ph·ªèng game (UI / CLI)** | L√† baseline ƒë·ªÉ test Minimax & ML | Tr∆∞·ªõc deadline th·ª±c 10 ng√†y ƒë·ªÉ l√†m b√°o c√°o + present
(2/12/2025) |
| **2. Minimax Agent (1 ng∆∞·ªùi)** | H√¢n | - X√¢y d·ª±ng **kh√¥ng gian tr·∫°ng th√°i** (state space) 
- Hi·ªán th·ª±c **Minimax + Alpha-Beta pruning**
 - T·ªëi ∆∞u hi·ªáu su·∫•t sinh n∆∞·ªõc ƒëi 
- X√¢y d·ª±ng **h√†m ƒë√°nh gi√° (heuristic)** | Minimax th·∫Øng random ‚â• 90% | Tr∆∞·ªõc deadline th·ª±c 10 ng√†y ƒë·ªÉ l√†m b√°o c√°o + present
(2/12/2025) |
| **3. Machine Learning Agent (1 ng∆∞·ªùi)** | Thanh | - Thu th·∫≠p dataset (t·ª´ Minimax vs Random ho·∫∑c t·ª´ log n·∫øu ch·ªçn ML l√† Supervised  Learning)
 - Thi·∫øt k·∫ø **feature vector** cho tr·∫°ng th√°i game 
- Hu·∫•n luy·ªán model (supervised learning ho·∫∑c reinforcement learning)
 - T√≠ch h·ª£p v√†o agent ƒë·ªÉ ch·ªçn n∆∞·ªõc ƒëi | ML th·∫Øng random ‚â• 60% | Tr∆∞·ªõc deadline th·ª±c 10 ng√†y ƒë·ªÉ l√†m b√°o c√°o + present
(2/12/2025) |
| **4. Documentation & Presentation (2 ng∆∞·ªùi)
- 1 ng∆∞·ªùi l√†m report
- 1 ng∆∞·ªùi l√†m slide + present video** | **- slide + present : L·ªôc

- report : Quang** | - T·ªïng h·ª£p t√†i li·ªáu k·ªπ thu·∫≠t 
- Vi·∫øt **report** & **slide**
- Chu·∫©n b·ªã **video thuy·∫øt tr√¨nh 15 ph√∫t** 
 | B√°o c√°o + tr√¨nh b√†y |  |
| **5. Integration & Testing(1 ng∆∞·ªùi)** | Long | - Ch·∫°y **test t·ª± ƒë·ªông** v·ªõi Random Agent 
- Ghi log k·∫øt qu·∫£ ƒë·ªÉ d√πng cho ML 
- T·ªëi ∆∞u performance
- Th·ªëng k√™ t·ªâ l·ªá th·∫Øng, bi·ªÉu ƒë·ªì k·∫øt qu·∫£ | ƒê·∫£m b·∫£o 3 agent t∆∞∆°ng t√°c ƒë√∫ng & ch·∫°y m∆∞·ª£t | Tr∆∞·ªõc deadline th·ª±c 10 ng√†y ƒë·ªÉ l√†m b√°o c√°o + present
(2/12/2025) |

**NOTE: Trong qu√° tr√¨nh hi·ªán th·ª±c c√°c agent,  ghi l·∫°i qu√° tr√¨nh hi·ªán th·ª±c v√† ki·ªÉm th·ª≠ ƒë·ªÉ ph·ª•c v·ª• vi·∫øt b√°o c√°o**